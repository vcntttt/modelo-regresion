
Esta guÃ­a explica **cada parte** del resultado entregado por `glm()` cuando ajustas un modelo logÃ­stico con `family = binomial`.

```log
Call:
glm(formula = diagnosis ~ ., family = binomial, data = df)

Coefficients:
                        Estimate Std. Error z value Pr(>|z|)    
(Intercept)             -7.35952   12.85259  -0.573   0.5669    
radius_mean             -2.04930    3.71588  -0.551   0.5813    
texture_mean             0.38473    0.06454   5.961  2.5e-09 ***
perimeter_mean          -0.07151    0.50516  -0.142   0.8874    
area_mean                0.03980    0.01674   2.377   0.0174 *  
smoothness_mean         76.43227   31.95492   2.392   0.0168 *  
compactness_mean        -1.46242   20.34249  -0.072   0.9427    
concavity_mean           8.46870    8.12003   1.043   0.2970    
concave.points_mean     66.82176   28.52910   2.342   0.0192 *  
symmetry_mean           16.27824   10.63059   1.531   0.1257    
fractal_dimension_mean -68.33703   85.55666  -0.799   0.4244    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 751.44  on 568  degrees of freedom
Residual deviance: 146.13  on 558  degrees of freedom
AIC: 168.13

Number of Fisher Scoring iterations: 9
```


## 1. Llamada al modelo (Call)

```r
Call:
glm(formula = diagnosis ~ ., family = binomial, data = df)
```

### Â¿QuÃ© significa?

* **`diagnosis ~ .`** â†’ la variable dependiente es `diagnosis`, y se usan **todas las demÃ¡s variables** del dataset como predictores.
* **`family = binomial`** â†’ es una **regresiÃ³n logÃ­stica** (logit).
* **`data = df`** â†’ el dataset utilizado se llama `df`.


## 2. Tabla de coeficientes

```r
Coefficients | Estimate | Std. Error | z value | Pr(>|z|)
```

Cada fila es un predictor.
Cada columna significa:
| Columna           | InterpretaciÃ³n                                      |
|-------------------|-----------------------------------------------------|
| **Estimate**      | Coeficiente estimado Î² (efecto sobre el logit).     |
| **Std. Error**    | Error estÃ¡ndar del coeficiente.                     |
| **z value**       | EstadÃ­stico z = Î² / SE.                             |
| **Pr(>|z|)**      | p-value del test Hâ‚€: Î² = 0.                          |
| **Signif. codes** | Asteriscos indicando nivel de significancia.        |
### InterpretaciÃ³n conceptual

* **Coeficiente positivo** â†’ aumenta la probabilidad del evento (`diagnosis = 1`).
* **Coeficiente negativo** â†’ disminuye esa probabilidad.
* **Asteriscos** â†’ indican quÃ© tan fuerte es la evidencia estadÃ­stica:

  * `***` p < 0.001 (muy fuerte)
  * `**` p < 0.01
  * `*` p < 0.05
  * `.` p < 0.1 (marginal)
  * sin sÃ­mbolo â†’ no significativo

### Ejemplo

```r
texture_mean   0.38473   0.06454   5.961   2.5e-09 ***
```

InterpretaciÃ³n:

* Un aumento en `texture_mean` incrementa la probabilidad del evento.
* p-value extremadamente bajo â†’ **muy significativo**.


## 3. Variables significativas vs. no significativas

### Significativas (aportan al modelo)

* `texture_mean`
* `area_mean`
* `smoothness_mean`
* `concave.points_mean`

### No significativas (no aportan de forma independiente)

* `radius_mean`
* `perimeter_mean`
* `compactness_mean`
* `concavity_mean`
* `symmetry_mean`
* `fractal_dimension_mean`

Esto puede ocurrir por:

* multicolinealidad (variables muy correlacionadas entre sÃ­),
* falta de aporte independiente,
* redundancia en los predictores.


## 4. ParÃ¡metro de dispersiÃ³n

```r
(Dispersion parameter for binomial family taken to be 1)
```

En regresiÃ³n logÃ­stica SIEMPRE es 1.
Nada que interpretar.


## 5. Deviance

```r
Null deviance: 751.44  on 568  degrees of freedom
Residual deviance: 146.13  on 558  degrees of freedom
```

### **Null deviance**

Error del modelo que NO usa predictores (solo la media).
Sirve como referencia.

### **Residual deviance**

Error del modelo con los predictores incluidos.

### Degrees of fredom

Los grados de libertad indican cuÃ¡ntos datos "libres" quedan para medir el error, despuÃ©s de descontar los parÃ¡metros que el modelo estima.

### InterpretaciÃ³n crucial

* Si la deviance disminuye muchÃ­simo â†’ el modelo **explica bien** la variable objetivo.
* AquÃ­ la reducciÃ³n es grande:

**751 â†’ 146**

Esto indica un modelo **muy bueno**.


## 6. AIC

```r
AIC: 168.13
```

El **Akaike Information Criterion**:

* Penaliza por complejidad.
* **MÃ¡s pequeÃ±o = mejor**, pero solo al comparar modelos.


## 7. Iteraciones del algoritmo

```r
Number of Fisher Scoring iterations: 9
```
* NÃºmero de pasos que necesitÃ³ el algoritmo para converger.
* Valores entre 4 y 15 son normales.


# ðŸ” Resumen simplificado

* El modelo estÃ¡ bien ajustado y convergiÃ³ sin problemas.
* La deviance baja muchÃ­simo â†’ **alto poder explicativo**.
* Solo algunas variables son realmente importantes estadÃ­sticamente.
* AIC=168 es bueno, pero Ãºtil solo comparado con otros modelos.
* Los coeficientes indican direcciÃ³n e impacto sobre la probabilidad.

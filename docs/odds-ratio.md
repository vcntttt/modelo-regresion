# ğŸ“Œ RegresiÃ³n LogÃ­stica: Odds, Logit, Odds Ratio, FunciÃ³n Objetivo y el Rol de Ridge/Lasso

Este documento resume de manera clara y tÃ©cnica cÃ³mo funciona la regresiÃ³n logÃ­stica desde su formulaciÃ³n matemÃ¡tica hasta el impacto directo de las penalizaciones Ridge y Lasso en el modelo y en la estimaciÃ³n de los coeficientes.



# ğŸ”¥ 1. Â¿QuÃ© son los *odds*?

Los **odds** son una forma alternativa de expresar una probabilidad.  
Mientras la probabilidad responde:  
> â€œÂ¿QuÃ© porcentaje de veces ocurre el evento?â€  

los **odds** responden:  
> â€œÂ¿CuÃ¡ntas veces es mÃ¡s probable que ocurra el evento que que NO ocurra?â€

MatemÃ¡ticamente:

$$
\text{odds} = \frac{p}{1 - p}
$$

donde:

- $p$ = probabilidad de que ocurra el evento  
- $1 - p$ = probabilidad de que NO ocurra

### ğŸ” InterpretaciÃ³n intuitiva

- **Odds = 1** â†’ el evento es igual de probable que ocurra que no ocurra  
  (equivale a $p = 0.5$)

- **Odds > 1** â†’ el evento es mÃ¡s probable que ocurra que no ocurra  
  (por ejemplo, si $\text{odds} = 4$, significa â€œ4 veces mÃ¡s probable que ocurraâ€)

- **Odds < 1** â†’ el evento es menos probable que ocurra  
  (por ejemplo, $\text{odds} = 0.25$ significa â€œ4 veces mÃ¡s probable que NO ocurraâ€)

### ğŸ”¢ Ejemplos simples

| Probabilidad $p$ | Probabilidad de NO ocurrir $1-p$ | Odds = $p/(1-p)$ | InterpretaciÃ³n |
|------------------|-----------------------------------|------------------|----------------|
| 0.5 | 0.5 | 1 | Igual probabilidad |
| 0.8 | 0.2 | 4 | 4 veces mÃ¡s probable que ocurra |
| 0.2 | 0.8 | 0.25 | 4 veces mÃ¡s probable que NO ocurra |

### ğŸ§© Rango posible
- $p \in (0,1)$  
- $\text{odds} \in (0, +\infty)$  

En resumen:  
**Los odds comparan la probabilidad de que algo ocurra con la probabilidad de que no ocurra**, y esa proporciÃ³n es la base del modelo logit en regresiÃ³n logÃ­stica.


# ğŸ”¥ 2. Â¿QuÃ© es el *logit*?

El **logit** es el logaritmo natural de los odds:

$$
\text{logit}(p) = \log\left( \frac{p}{1-p} \right)
$$

Ventajas del logit:
- Convierte $p \in (0,1)$ en un valor real $(-\infty, +\infty)$.
- Permite modelar la probabilidad con una funciÃ³n **lineal** en los predictores:

$$
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k.
$$

En resumen:
- La regresiÃ³n logÃ­stica **no modela la probabilidad directamente**, sino el **logit de $p$**.



# ğŸ”¥ 3. Â¿QuÃ© es el *odds ratio (OR)*?

El **odds ratio** mide cuÃ¡nto cambian los odds cuando un predictor $X_j$ aumenta en una unidad:

$$
\text{OR}_j = e^{\beta_j}
$$

InterpretaciÃ³n:
- Si $\beta_j = 0.7$:  
  $\text{OR} = e^{0.7} \approx 2.01$ â†’ subir $X_j$ una unidad multiplica los odds por ~2.
- Si OR = 1 â†’ sin efecto.  
- Si OR > 1 â†’ aumenta odds.  
- Si OR < 1 â†’ disminuye odds.

El OR **no es una probabilidad**, sino una razÃ³n de odds.



# ğŸ”¥ 4. Modelo y FunciÃ³n Objetivo en RegresiÃ³n LogÃ­stica

## âœ”ï¸ Modelo (relaciÃ³n entre $X$ y la probabilidad)

$$
\log\left( \frac{p}{1-p} \right) = X\beta
$$

Equivalente:

$$
p = \frac{1}{1 + e^{-X\beta}}.
$$

Este es el **modelo logit**, que *no cambia* aunque se use regularizaciÃ³n.



## âœ”ï¸ FunciÃ³n objetivo (lo que realmente optimiza el algoritmo)

La regresiÃ³n logÃ­stica estima los $\beta$ **maximizando la verosimilitud**:

$$
L(\beta) = \prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{1-y_i}.
$$

Se usa casi siempre la **log-verosimilitud**:

$$
\ell(\beta) =
\sum_{i=1}^{n}
\left[
y_i \log(p_i) + (1 - y_i)\log(1-p_i)
\right].
$$

Este es el criterio interno de `glm()` en R.



# ğŸ”¥ 5. Resumen Conceptual (versiÃ³n tabla)

| Concepto | DefiniciÃ³n | FÃ³rmula | Rol |
|---------|------------|---------|-----|
| **Odds** | RazÃ³n $p$ vs $1-p$ | $p/(1-p)$ | Intensidad del evento |
| **Logit** | Logaritmo de odds | $\log(p/(1-p))$ | Linealiza el modelo |
| **Odds Ratio** | Cambio multiplicativo en odds | $e^{\beta_j}$ | InterpretaciÃ³n |
| **Modelo** | RelaciÃ³n $X \to p$ | $\log(p/(1-p)) = X\beta$ | Forma funcional |
| **FunciÃ³n objetivo** | Lo que se optimiza | $\max \ell(\beta)$ | EstimaciÃ³n |



# ğŸ”¥ 6. Ridge & Lasso: QuÃ© *sÃ­* cambian y quÃ© *no*

### âŒ Lo que **NO** cambian:
El modelo logit:

$$
\log\left( \frac{p}{1-p} \right) = X\beta.
$$

La forma de $p$:

$$
p = \frac{1}{1 + e^{-X\beta}}.
$$

### âœ”ï¸ Lo que **SÃ** cambian:
La **funciÃ³n objetivo usada para estimar los coeficientes**.



# ğŸ”¥ 7. FunciÃ³n Objetivo con RegularizaciÃ³n

La forma penalizada general es:

$$
\text{Objetivo} = -\ell(\beta) + \lambda \cdot Pen(\beta).
$$

$\lambda$ controla cuÃ¡nto se penaliza la magnitud de los coeficientes.



# ğŸ”¥ 8. Ridge (PenalizaciÃ³n L2)

La penalizaciÃ³n es la suma de cuadrados:

$$
Pen_{ridge}(\beta) = \sum_{j=1}^p \beta_j^2.
$$

FunciÃ³n objetivo:

$$
-\ell(\beta) + \lambda \sum_{j=1}^p \beta_j^2.
$$

### Efectos:
- Los coeficientes $\beta$ se **reducen en magnitud**.
- **Nunca llegan a cero**.
- Es excelente contra multicolinealidad.
- Hace el modelo mÃ¡s estable (menos varianza).

### Impacto en el logit:
$$
\log\left( \frac{p}{1-p} \right) = X\beta_{ridge}
$$

Con $\beta_{ridge}$ mÃ¡s pequeÃ±os:

$$
OR_j = e^{\beta_{j,ridge}} \approx 1.
$$



# ğŸ”¥ 9. Lasso (PenalizaciÃ³n L1)

PenalizaciÃ³n como suma de valores absolutos:

$$
Pen_{lasso}(\beta) = \sum_{j=1}^p |\beta_j|.
$$

FunciÃ³n objetivo:

$$
-\ell(\beta) + \lambda \sum_{j=1}^p |\beta_j|.
$$

### Efectos:
- Algunos coeficientes quedan **exactamente en cero**.
- Realiza selecciÃ³n automÃ¡tica de variables.
- Reduce complejidad del modelo.

### Impacto en el logit:
$$
\log\left( \frac{p}{1-p} \right) = X\beta_{lasso}
$$

Si un coeficiente es 0:

$$
OR_j = e^0 = 1.
$$

La variable queda fuera del modelo.



# ğŸ”¥ 10. ComparaciÃ³n Global

| Aspecto | LogÃ­stica EstÃ¡ndar | Ridge | Lasso |
|--------|--------------------|--------|--------|
| Coeficientes | Sin restricciÃ³n | Reducidos | Reducidos o = 0 |
| SelecciÃ³n variables | No | No | SÃ­ |
| Estabilidad | Media | Alta | Alta |
| Multicolinealidad | No resuelve | SÃ­ | Parcial |
| InterpretaciÃ³n | Completa | Similar pero con efectos reducidos | MÃ¡s simple |



# ğŸ”¥ 11. Resumen Final en 30 segundos

- El **modelo logit no cambia jamÃ¡s**.  
- Lo que cambia es **cÃ³mo estimamos los coeficientes**.
- La funciÃ³n objetivo pasa de maximizar $\ell(\beta)$ a maximizar:

  - Ridge: $\ell(\beta) - \lambda \sum \beta_j^2$
  - Lasso: $\ell(\beta) - \lambda \sum |\beta_j|$

- Ridge encoge los coeficientes.  
- Lasso encoge *y elimina* coeficientes.  
- Los OR se acercan a 1 (o quedan en 1 si el coeficiente se hace cero).  
- La estabilidad del modelo mejora con regularizaciÃ³n.



# ðŸ”¥ 1. Â¿QuÃ© son los **odds**?

Para un evento con probabilidad $p$, los **odds** (o razones de momios) se definen como:

$$
\text{odds} = \frac{p}{1-p}
$$

InterpretaciÃ³n:
- Si $p = 0.5$, los odds = 1 (igual de probable que ocurra que no ocurra).
- Si $p = 0.8$, los odds = 4 (4 veces mÃ¡s probable que ocurra).
- Si $p = 0.2$, los odds = 0.25 (4 veces mÃ¡s probable que NO ocurra).

Es una escala asimÃ©trica:
- $p \in (0,1)$
- $odds \in (0, +\infty)$



# ðŸ”¥ 2. Â¿QuÃ© es el **logit**?

El **logit** es el logaritmo natural de los odds:

$$
\text{logit}(p) = \log \left( \frac{p}{1-p} \right)
$$

Â¿Por quÃ© usar logit?
- Transforma $p \in (0,1)$ a todo $\mathbb{R}$
- Permite modelar la probabilidad con una funciÃ³n **lineal** en los predictores:

$$
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k
$$

Es decir:
- La regresiÃ³n logÃ­stica NO modela directamente $p$
- Modela el **logit de p** como una combinaciÃ³n lineal de X



# ðŸ”¥ 3. Â¿QuÃ© es el **odds ratio (OR)**?

El OR mide **cuÃ¡nto cambian los odds** cuando un predictor $X_j$ aumenta en una unidad.

En regresiÃ³n logÃ­stica:

$$
\text{OR}_j = e^{\beta_j}
$$

InterpretaciÃ³n:
- Si $\beta_j = 0.7$, entonces OR = $e^{0.7} \approx 2.01$
    - Los odds se multiplican por 2.01 cuando $X_j$ sube una unidad.
- Si OR = 1 â†’ no hay efecto.
- Si OR > 1 â†’ aumenta los odds.
- Si OR < 1 â†’ disminuye los odds.

**Importante:**  
El OR no son probabilidades, es una razÃ³n de odds.



# ðŸ”¥ 4. Â¿CuÃ¡l es la **funciÃ³n objetivo** en la regresiÃ³n logÃ­stica?

La **funciÃ³n objetivo NO es el modelo** en sÃ­.  
Son **dos cosas distintas**:

## âœ”ï¸ **El modelo**  
Es la ecuaciÃ³n que relaciona los covariables con el logit:

$$
\log\left( \frac{p}{1-p} \right) = X\beta
$$

O equivalente:

$$
p = \frac{1}{1 + e^{-X\beta}}
$$



## âœ”ï¸ **La funciÃ³n objetivo (objective function)**  
Es la funciÃ³n que el algoritmo optimiza para encontrar los $\beta$.

En regresiÃ³n logÃ­stica, la funciÃ³n objetivo es:

### ðŸŽ¯ **Maximizar la verosimilitud**  
(= elegir los parÃ¡metros que hacen mÃ¡s probable observar los datos)

Para datos binarios $y_i \in \{0,1\}$:

$$
L(\beta) = \prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{1-y_i}
$$

Y casi siempre trabajamos con la log-verosimilitud:

$$
\ell(\beta) = \sum_{i=1}^{n} \left[
y_i \log(p_i) + (1 - y_i)\log(1-p_i)
\right]
$$

**Esto es lo que realmente maximiza `glm()` en R o cualquier software.**



# ðŸ”¥ 5. Resumen Conceptual Final

| Concepto | QuÃ© es | FÃ³rmula | Rol en regresiÃ³n logÃ­stica |
|---------|--------|----------|-----------------------------|
| **Odds** | Ratio "probabilidad vs no probabilidad" | $p/(1-p)$ | Representa la intensidad del evento |
| **Logit** | Logaritmo de los odds | $\log(p/(1-p))$ | Permite linealizar y usar regresiÃ³n |
| **OR** | Impacto multiplicativo de un predictor en los odds | $e^{\beta_j}$ | InterpretaciÃ³n de efectos |
| **Modelo** | EcuaciÃ³n que define cÃ³mo X influye en p | $\log(p/(1-p)) = X \beta$ | Describe la relaciÃ³n |
| **FunciÃ³n objetivo** | Lo que se optimiza para estimar los $\beta$ | maximizar $\ell(\beta)$ | Estima los parÃ¡metros |

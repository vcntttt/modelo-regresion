# üìò Regularizaci√≥n en Regresi√≥n Log√≠stica: LASSO y Ridge  

Este documento presenta una explicaci√≥n completa de las t√©cnicas de regularizaci√≥n **LASSO** y **Ridge**, por qu√© es adecuado utilizarlas en el dataset WDBC, c√≥mo funcionan matem√°ticamente, c√≥mo se aplicaron en nuestro an√°lisis y c√≥mo interpretar los resultados obtenidos.  



# 1. ¬øQu√© es la regularizaci√≥n?

En modelos estad√≠sticos, especialmente en regresi√≥n log√≠stica, es frecuente encontrarnos con:

- variables altamente correlacionadas (multicolinealidad),
- coeficientes inestables,
- sobreajuste (overfitting),
- modelos innecesariamente complejos.

La **regularizaci√≥n** introduce una penalizaci√≥n adicional en la funci√≥n objetivo del modelo con el prop√≥sito de:

1. **Evitar sobreajuste**,  
2. **Estabilizar los coeficientes**,  
3. **Reducir complejidad**,  
4. **Mejorar la capacidad predictiva en datos nuevos**,  
5. **Seleccionar autom√°ticamente variables relevantes** (solo LASSO).  

Existen dos m√©todos principales: **Ridge** y **LASSO**.



# 2. Regresi√≥n Ridge (Penalizaci√≥n L2)

## 2.1 ¬øQu√© es?

La regresi√≥n Ridge agrega una penalizaci√≥n proporcional a la **suma de los cuadrados** de los coeficientes:

$$
\lambda \sum_{j=1}^{p} \beta_j^2
$$

Esto es conocido como **penalizaci√≥n L2**.

## 2.2 ¬øQu√© hace Ridge?

- Reduce (‚Äúencoge‚Äù) los coeficientes hacia 0, pero **nunca los hace exactamente cero**.  
- Mantiene todas las variables y **reparte** el efecto entre predictores correlacionados.  
- Produce **coeficientes estables**, especialmente √∫til cuando existen cl√∫steres de variables altamente correlacionadas.

## 2.3 ¬øCu√°ndo es ideal usar Ridge?

- Cuando queremos mantener toda la informaci√≥n.  
- Cuando existe multicolinealidad extrema (como en WDBC).  
- Cuando la interpretabilidad no requiere eliminar predictores.  
- Cuando el objetivo es **estabilidad** y no selecci√≥n de variables.



# 3. Regresi√≥n LASSO (Penalizaci√≥n L1)

## 3.1 ¬øQu√© es?

LASSO agrega una penalizaci√≥n proporcional a la **suma de valores absolutos** de los coeficientes:

$$
\lambda \sum_{j=1}^{p} |\beta_j|
$$

Esto es la **penalizaci√≥n L1**.

## 3.2 ¬øQu√© hace LASSO?

- Reduce coeficientes hacia cero.  
- Puede hacer que algunos coeficientes se vuelvan **exactamente cero**.  
- Realiza **selecci√≥n autom√°tica de variables**.  
- Produce modelos **parsimoniosos y altamente interpretables**.

## 3.3 ¬øCu√°ndo usar LASSO?

- Cuando queremos un modelo m√°s simple.  
- Cuando hay variables redundantes.  
- Cuando la interpretabilidad es prioritaria.  
- Cuando se necesitan menos predictores sin perder desempe√±o.



# 4. ¬øPor qu√© usar LASSO y Ridge en WDBC?

El dataset WDBC contiene:

- 10 variables `_mean`
- numerosos subgrupos altamente correlacionados:
  - tama√±o: radius_mean, area_mean, perimeter_mean  
  - forma: concavity_mean, concave.points_mean, compactness_mean  
  - propiedades finas: texture_mean, smoothness_mean, symmetry_mean, fractal_dimension_mean

### Problemas generados por esta estructura:

- **Multicolinealidad severa** ‚Üí coeficientes inflados e inestables.  
- **Redundancia** ‚Üí varias variables describen lo mismo.  
- **Modelos sensibles a peque√±as perturbaciones**.

Por estas razones, aplicar regularizaci√≥n es completamente apropiado:

| Objetivo | LASSO | Ridge |
|---|:--:|:--:|
| Seleccionar predictores |  S√≠ |  No |
| Estabilizar coeficientes |  Moderado |  Excelente |
| Manejar colinealidad |  Depende |  Ideal |
| Obtener modelo compacto |  Alto |  No |



# 5. ¬øC√≥mo funcionan Ridge y LASSO matem√°ticamente?

Partimos de la regresi√≥n log√≠stica cl√°sica, cuya funci√≥n a maximizar es la log-verosimilitud:

$$
\ell(\beta)
$$

Con regularizaci√≥n, maximizamos:

### **Ridge:**
$$
\ell(\beta) - \lambda \sum \beta_j^2
$$

### **LASSO:**
$$
\ell(\beta) - \lambda \sum |\beta_j|
$$

El par√°metro **Œª** controla cu√°nta penalizaci√≥n aplicamos.

- Œª peque√±o ‚Üí modelo cercano al completo.  
- Œª grande ‚Üí coeficientes fuertemente penalizados.  
- En LASSO, Œª grande ‚Üí coeficientes EXACTAMENTE cero.

Los valores √≥ptimos de Œª se seleccionan autom√°ticamente mediante **validaci√≥n cruzada** (CV).



# 6. C√≥mo aplicamos LASSO y Ridge en WDBC

## 6.1 Preparaci√≥n de datos

- Se seleccionaron solo variables `_mean` para evitar redundancia.  
- Se transform√≥ diagnosis en 0/1.  
- Se cre√≥ una matriz num√©rica `X` y un vector `y`.  

## 6.2 Ajuste de modelos con `glmnet`

### **LASSO:**  
```r
cv_lasso <- cv.glmnet(X, y, family="binomial", alpha=1)
```

### **Ridge:**

```r
cv_ridge <- cv.glmnet(X, y, family="binomial", alpha=0)
```

En ambos casos se usaron:

* 10-fold cross-validation
* lambda.min (mejor error)
* lambda.1se (modelo m√°s simple dentro de 1 desviaci√≥n est√°ndar)

## 6.3 Conversi√≥n a modelos `glm` cl√°sicos

Para poder:

* obtener AIC,
* obtener pseudo-R¬≤,
* comparar con modelos manuales,

reconstruimos modelos glm usando las variables seleccionadas por LASSO y Ridge.
## 7. Interpretaci√≥n de los resultados obtenidos

### 7.1 Comparaci√≥n de AIC

| Modelo | AIC | Comentario |
| --- | --- | --- |
| **LASSO (Œª.min)** | **166.35** | Mejor ajuste (Œª = 0.0063) |
| Modelo completo | 168.13 | Referencia sin penalizaci√≥n |
| Ridge (Œª.min / Œª.1se) | 168.13 | Replica el completo con coeficientes suavizados |
| **LASSO (1-SE)** | 172.35 | Compacto (Œª = 0.0403) |
| Modelo A (manual) | 172.38 | 3 vars, muy interpretable |
| Modelos B‚ÄìD | 209‚Äì215 | Pierden demasiada informaci√≥n |

### Interpretaci√≥n

* **LASSO Œª.min** ofrece el mejor desempe√±o global.
* **LASSO 1-SE** mantiene un AIC competitivo con menos variables.
* **Ridge** confirma el buen ajuste del modelo completo pero no reduce dimensionalidad.
* **Modelo A** se conserva como la opci√≥n manual m√°s interpretable.


## 7.2 Selecci√≥n de variables

### üîµ LASSO (Œª.min): 6 variables

`texture_mean`, `area_mean`, `smoothness_mean`, `concavity_mean`, `concave.points_mean`, `symmetry_mean`.
Mejor AIC, mantiene variables de forma y textura, descarta radio/per√≠metro.

### üîµ LASSO (1-SE): 4 variables

`radius_mean`, `texture_mean`, `perimeter_mean`, `concave.points_mean`.
Modelo compacto dentro de 1-SE del m√≠nimo de CV.

### üîµ Ridge: 10 variables

Conserva todas las variables `_mean`; el aporte es estabilizar coeficientes ante colinealidad extrema.

## 7.3 Pseudo-R¬≤

* Modelo completo y Ridge: McFadden R¬≤ ‚âà **0.806**.
* LASSO Œª.min: ‚âà **0.797**.
* LASSO 1-SE: ‚âà **0.784**.
* Modelo A: ‚âà **0.781**.

### Interpretaci√≥n:

* Ridge confirma que el modelo completo est√° bien ajustado pero sin selecci√≥n de variables.
* LASSO mejora AIC con p√©rdida m√≠nima de R¬≤.
* El modelo A sigue siendo eficiente y explicativo pese a su simplicidad.

## 7.4 Accuracy en datos de entrenamiento

Como complemento al AIC y a los pseudo‚ÄëR¬≤, se evalu√≥ la **accuracy** (proporci√≥n de clasificaciones correctas) de todos los modelos sobre los mismos datos usados para el ajuste. Los resultados principales fueron:

- GLM completo: ‚âà **0.949**
- Post‚ÄëLASSO (Œª.min): ‚âà **0.946**
- LASSO penalizado (Œª.min): ‚âà **0.944**
- Modelo A y post‚ÄëLASSO (Œª.1se): ‚âà **0.942**
- Ridge penalizado (Œª.min): ‚âà **0.940**

Modelos m√°s simplificados (B, C, D y Ridge Œª.1se) caen al rango **0.92‚Äì0.93**, lo que indica una ligera p√©rdida de desempe√±o al reducir demasiado la complejidad.  
Estos valores se obtuvieron con el script `lasso.r` y quedan registrados en `logs/05_accuracy_modelos.log`; al estar calculados en el conjunto de entrenamiento deben interpretarse como un l√≠mite superior del desempe√±o esperable en datos nuevos.



# 8. Conclusiones finales

## ‚úî ¬øFue buena idea usar LASSO y Ridge?

S√≠. Ambos m√©todos:

* mitigan la colinealidad severa,
* estabilizan coeficientes (Ridge),
* seleccionan variables (LASSO),
* permiten contrastar parsimonia vs. desempe√±o de forma autom√°tica.

## ‚úî ¬øQu√© modelo es mejor?

Depende del objetivo:

### ü•á **Mejor modelo predictivo global:**

**LASSO (Œª.min)** ‚Äî AIC ‚âà 166.35

### üéØ **Mejor modelo parsimonioso autom√°tico:**

**LASSO (1-SE)** ‚Äî 4 variables, AIC competitivo

### üìò **Mejor modelo manual interpretativo:**

**Modelo A** ‚Äî 3 variables bien definidas

### ‚öôÔ∏è **Modelo m√°s estable:**

**Ridge** ‚Äî equivalente al modelo completo, coeficientes suavizados

## ‚úî ¬øQu√© aprendemos?

* LASSO identifica redundancias y simplifica el modelo.
* Ridge confirma la relevancia general de todas las variables.
* El dataset WDBC es muy informativo; incluso modelos simples son excelentes.
* Parsimonia y rendimiento no est√°n peleados:

  * Modelo A
  * LASSO 1-SE
  * LASSO Œª.min

representan puntos √≥ptimos en el trade-off.

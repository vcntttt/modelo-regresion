# üìò Regularizaci√≥n en Regresi√≥n Log√≠stica: LASSO y Ridge  

Este documento presenta una explicaci√≥n completa de las t√©cnicas de regularizaci√≥n **LASSO** y **Ridge**, por qu√© es adecuado utilizarlas en el dataset WDBC, c√≥mo funcionan matem√°ticamente, c√≥mo se aplicaron en nuestro an√°lisis y c√≥mo interpretar los resultados obtenidos.  



# 1. ¬øQu√© es la regularizaci√≥n?

En modelos estad√≠sticos, especialmente en regresi√≥n log√≠stica, es frecuente encontrarnos con:

- variables altamente correlacionadas (multicolinealidad),
- coeficientes inestables,
- sobreajuste (overfitting),
- modelos innecesariamente complejos.

La **regularizaci√≥n** introduce una penalizaci√≥n adicional en la funci√≥n objetivo del modelo con el prop√≥sito de:

1. **Evitar sobreajuste**,  
2. **Estabilizar los coeficientes**,  
3. **Reducir complejidad**,  
4. **Mejorar la capacidad predictiva en datos nuevos**,  
5. **Seleccionar autom√°ticamente variables relevantes** (solo LASSO).  

Existen dos m√©todos principales: **Ridge** y **LASSO**.



# 2. Regresi√≥n Ridge (Penalizaci√≥n L2)

## 2.1 ¬øQu√© es?

La regresi√≥n Ridge agrega una penalizaci√≥n proporcional a la **suma de los cuadrados** de los coeficientes:

$$
\lambda \sum_{j=1}^{p} \beta_j^2
$$

Esto es conocido como **penalizaci√≥n L2**.

## 2.2 ¬øQu√© hace Ridge?

- Reduce (‚Äúencoge‚Äù) los coeficientes hacia 0, pero **nunca los hace exactamente cero**.  
- Mantiene todas las variables y **reparte** el efecto entre predictores correlacionados.  
- Produce **coeficientes estables**, especialmente √∫til cuando existen cl√∫steres de variables altamente correlacionadas.

## 2.3 ¬øCu√°ndo es ideal usar Ridge?

- Cuando queremos mantener toda la informaci√≥n.  
- Cuando existe multicolinealidad extrema (como en WDBC).  
- Cuando la interpretabilidad no requiere eliminar predictores.  
- Cuando el objetivo es **estabilidad** y no selecci√≥n de variables.



# 3. Regresi√≥n LASSO (Penalizaci√≥n L1)

## 3.1 ¬øQu√© es?

LASSO agrega una penalizaci√≥n proporcional a la **suma de valores absolutos** de los coeficientes:

$$
\lambda \sum_{j=1}^{p} |\beta_j|
$$

Esto es la **penalizaci√≥n L1**.

## 3.2 ¬øQu√© hace LASSO?

- Reduce coeficientes hacia cero.  
- Puede hacer que algunos coeficientes se vuelvan **exactamente cero**.  
- Realiza **selecci√≥n autom√°tica de variables**.  
- Produce modelos **parsimoniosos y altamente interpretables**.

## 3.3 ¬øCu√°ndo usar LASSO?

- Cuando queremos un modelo m√°s simple.  
- Cuando hay variables redundantes.  
- Cuando la interpretabilidad es prioritaria.  
- Cuando se necesitan menos predictores sin perder desempe√±o.



# 4. ¬øPor qu√© usar LASSO y Ridge en WDBC?

El dataset WDBC contiene:

- 10 variables `_mean`
- numerosos subgrupos altamente correlacionados:
  - tama√±o: radius_mean, area_mean, perimeter_mean  
  - forma: concavity_mean, concave.points_mean, compactness_mean  
  - propiedades finas: texture_mean, smoothness_mean, symmetry_mean, fractal_dimension_mean

### Problemas generados por esta estructura:

- **Multicolinealidad severa** ‚Üí coeficientes inflados e inestables.  
- **Redundancia** ‚Üí varias variables describen lo mismo.  
- **Modelos sensibles a peque√±as perturbaciones**.

Por estas razones, aplicar regularizaci√≥n es completamente apropiado:

| Objetivo | LASSO | Ridge |
|---|:--:|:--:|
| Seleccionar predictores |  S√≠ |  No |
| Estabilizar coeficientes |  Moderado |  Excelente |
| Manejar colinealidad |  Depende |  Ideal |
| Obtener modelo compacto |  Alto |  No |



# 5. ¬øC√≥mo funcionan Ridge y LASSO matem√°ticamente?

Partimos de la regresi√≥n log√≠stica cl√°sica, cuya funci√≥n a maximizar es la log-verosimilitud:

$$
\ell(\beta)
$$

Con regularizaci√≥n, maximizamos:

### **Ridge:**
$$
\ell(\beta) - \lambda \sum \beta_j^2
$$

### **LASSO:**
$$
\ell(\beta) - \lambda \sum |\beta_j|
$$

El par√°metro **Œª** controla cu√°nta penalizaci√≥n aplicamos.

- Œª peque√±o ‚Üí modelo cercano al completo.  
- Œª grande ‚Üí coeficientes fuertemente penalizados.  
- En LASSO, Œª grande ‚Üí coeficientes EXACTAMENTE cero.

Los valores √≥ptimos de Œª se seleccionan autom√°ticamente mediante **validaci√≥n cruzada** (CV).



# 6. C√≥mo aplicamos LASSO y Ridge en WDBC

## 6.1 Preparaci√≥n de datos

- Se seleccionaron solo variables `_mean` para evitar redundancia.  
- Se transform√≥ diagnosis en 0/1.  
- Se cre√≥ una matriz num√©rica `X` y un vector `y`.  

## 6.2 Ajuste de modelos con `glmnet`

### **LASSO:**  
```r
cv_lasso <- cv.glmnet(X, y, family="binomial", alpha=1)
```

### **Ridge:**

```r
cv_ridge <- cv.glmnet(X, y, family="binomial", alpha=0)
```

En ambos casos se usaron:

* 10-fold cross-validation
* lambda.min (mejor error)
* lambda.1se (modelo m√°s simple dentro de 1 desviaci√≥n est√°ndar)

## 6.3 Conversi√≥n a modelos `glm` cl√°sicos

Para poder:

* obtener AIC,
* obtener pseudo-R¬≤,
* comparar con modelos manuales,

reconstruimos modelos glm usando las variables seleccionadas por LASSO y Ridge.
## 7. Interpretaci√≥n de los resultados obtenidos

### 7.1 Comparaci√≥n de AIC

| Modelo | AIC |
| --- | --- |
| **LASSO (Œª.min)** | **166.19** ‚Üê mejor |
| Modelo completo | 168.13 |
| Ridge / Ridge 1-SE | 168.13 |
| **LASSO (1-SE)** | 169.38 |
| Modelo A (manual) | 172.38 |
| Otros modelos | Peor |

### Interpretaci√≥n

* **LASSO Œª.min** genera el mejor modelo predictivo.
* **LASSO 1-SE** ofrece un modelo m√°s parsimonioso (5 variables) con un AIC competitivo.
* **Ridge** no mejora el modelo completo; lo reproduce.
* **Modelo A** sigue siendo el referente manual m√°s interpretable.


## 7.2 Selecci√≥n de variables

### üîµ LASSO (Œª.min): 7 variables

Elimina solo 3 variables redundantes.
Logra el mejor AIC.

### üîµ LASSO (1-SE): 5 variables

Versi√≥n autom√°tica del Modelo A + 2 variables complementarias.
Excelente equilibrio simplicidad‚Äìdesempe√±o.

### üîµ Ridge: 10 variables

No elimina ninguna ‚Üí estabiliza coeficientes.
Confirm√≥ que ninguna variable es irrelevante seg√∫n CV.

## 7.3 Pseudo-R¬≤

* El modelo completo y Ridge alcanzan McFadden R¬≤ ‚âà **0.806**
* LASSO Œª.min ‚âà **0.800**
* LASSO 1-SE ‚âà **0.790**
* Modelo A ‚âà **0.781**

### Interpretaci√≥n:

* Ridge confirma que el modelo completo est√° bien ajustado.
* LASSO mejora AIC sin p√©rdida relevante de R¬≤.
* El modelo A sigue siendo muy eficiente pese a su simplicidad.



# 8. Conclusiones finales

## ‚úî ¬øFue buena idea usar LASSO y Ridge?

S√≠. Ambos m√©todos:

* solucionan la colinealidad severa,
* estabilizan coeficientes,
* permiten comparar parsimonia vs desempe√±o,
* verifican emp√≠ricamente la estructura del dataset.

## ‚úî ¬øQu√© modelo es mejor?

Depende del objetivo:

### ü•á **Mejor modelo predictivo global:**

**LASSO (Œª.min)** ‚Äî AIC = 166.19

### üéØ **Mejor modelo parsimonioso autom√°tico:**

**LASSO (1-SE)** ‚Äî 5 variables, AIC competitivo

### üìò **Mejor modelo manual interpretativo:**

**Modelo A** ‚Äî 3 variables bien definidas

### ‚öôÔ∏è **Modelo m√°s estable:**

**Ridge** ‚Äî equivalente al modelo completo, coeficientes suaves

## ‚úî ¬øQu√© aprendemos?

* LASSO identifica redundancias y simplifica el modelo.
* Ridge confirma la relevancia general de todas las variables.
* El dataset WDBC es muy informativo; incluso modelos simples son excelentes.
* Parsimonia y rendimiento no est√°n peleados:

  * Modelo A
  * LASSO 1-SE
  * LASSO Œª.min

representan puntos √≥ptimos en el trade-off.


# ğŸ“˜ Verosimilitud y Log-Verosimilitud  

## ğŸ” 1. Â¿QuÃ© es la **verosimilitud**?

La **verosimilitud** mide **quÃ© tan plausible es un valor del parÃ¡metro** dado que ya observamos los datos.  
No es â€œprobabilidad del parÃ¡metroâ€, sino **probabilidad de los datos suponiendo un parÃ¡metro**.

Si los datos son independientes:

$$
L(\theta) = \prod_{i=1}^n f(x_i \mid \theta)
$$

**Idea clave:**  
> Un mejor modelo (o parÃ¡metro) es aquel que hace mÃ¡s probable haber observado los datos que tenemos.


## ğŸ¯ 2. Â¿Para quÃ© se usa?

- **EstimaciÃ³n de parÃ¡metros** (MÃ¡xima Verosimilitud, MLE)  
  $$
  \hat{\theta} = \arg\max_\theta L(\theta)
  $$

- **ComparaciÃ³n entre modelos** (AIC, BIC, LRT)

- **Fundamento de los GLM** (logÃ­stica, Poisson, etc.)

- **ConstrucciÃ³n de intervalos y pruebas estadÃ­sticas**  
  (test de razÃ³n de verosimilitudes, devianza)


## ğŸ“Œ 3. Ejemplo

Datos Bernoulli:
```
1 0 1 1 1 0 (4 Ã©xitos, 2 fracasos)
```


$$
L(p)= p^4 (1-p)^2
$$

El parÃ¡metro que maximiza esta expresiÃ³n es:

$$
\hat{p} = \frac{4}{6}
$$


## ğŸ”¥ 4. Â¿QuÃ© es la **log-verosimilitud**?

Simplemente:

$$
\ell(\theta) = \log L(\theta)
$$

Convierte productos enormes en sumas manejables:

$$
\ell(\theta)= \sum_{i=1}^n \log f(x_i \mid \theta)
$$


## ğŸŒŸ 5. Â¿Por quÃ© se usa tanto?

### âœ” Evita underflow numÃ©rico  
Probabilidades muy pequeÃ±as multiplicadas â†’ 0 para un computador.

### âœ” Simplifica derivadas  
Para maximizar, derivamos $\ell(\theta)$, no $L(\theta)$.

### âœ” Permite pruebas y criterios modernos  
- AIC usa:  
  $$
  AIC = -2\,\ell(\hat\theta) + 2k
  $$
- Devianza en GLM  
- Test de razÃ³n de verosimilitudes (LRT)

### âœ” Da propiedades asintÃ³ticas importantes  
La forma de $\ell(\theta)$ determina la precisiÃ³n del estimador.


## ğŸ§  6. Resumen esencial (para recordar siempre)

- **Verosimilitud:** compatibilidad entre datos y parÃ¡metros.  
- **Log-verosimilitud:** logaritmo de la verosimilitud â†’ mÃ¡s estable y fÃ¡cil.  
- **MLE:** parÃ¡metro que maximiza la log-verosimilitud.  
- **Base de todo:** AIC, GLM, logistic regression, devianza, LRT.

Si entiendes estos conceptos, gran parte de la estadÃ­stica inferencial moderna se vuelve mucho mÃ¡s intuitiva.




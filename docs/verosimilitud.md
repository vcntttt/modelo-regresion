# ğŸ“˜ Verosimilitud y Log-Verosimilitud  
Apunte claro para comprender la base de la inferencia estadÃ­stica


## ğŸ§© 0. Â¿QuÃ© es un **parÃ¡metro**?

Un **parÃ¡metro** es un nÃºmero desconocido que caracteriza a un modelo o distribuciÃ³n.

Ejemplos:
- Bernoulli/Binomial â†’ **p** (probabilidad de Ã©xito)  
- Normal â†’ **Î¼** (media), **Ïƒ** (desviaciÃ³n estÃ¡ndar)  
- Poisson â†’ **Î»** (tasa)  
- LogÃ­stica/GLM â†’ **Î²â‚€, Î²â‚, â€¦** (coeficientes)

Los parÃ¡metros **no se observan directamente**: son propiedades del proceso que genera los datos.  
La estadÃ­stica busca **estimarlos** usando una muestra.


## ğŸ” 1. Â¿QuÃ© es la **verosimilitud**?

La **verosimilitud** mide **quÃ© tan plausible es un valor del parÃ¡metro** dado que ya observamos los datos.  
Es decir:

> No es â€œprobabilidad del parÃ¡metroâ€, sino **probabilidad de los datos suponiendo un parÃ¡metro**.

Formalmente, si los datos son independientes:

$$
L(\theta) = \prod_{i=1}^n f(x_i \mid \theta)
$$

**InterpretaciÃ³n clave:**  
> Un parÃ¡metro es mejor que otro si produce datos mÃ¡s parecidos a los que realmente observamos.


## ğŸ§  1.1. Â¿QuÃ© estamos haciendo realmente?

- Los **datos ya estÃ¡n fijados** (lo observado no cambia).  
- El parÃ¡metro **no lo conocemos**.  
- Probamos distintos valores del parÃ¡metro y preguntamos:  
  **Â¿QuÃ© tan coherente es este valor con los datos reales?**

La verosimilitud es justamente esa funciÃ³n de â€œcoherenciaâ€.


## ğŸ¯ 2. Â¿Para quÃ© se usa?

- **EstimaciÃ³n de parÃ¡metros** (MÃ¡xima Verosimilitud, MLE)  
  $$
  \hat{\theta} = \arg\max_\theta L(\theta)
  $$

- **ComparaciÃ³n entre modelos**  
  (AIC, BIC, test de razÃ³n de verosimilitudes)

- **Fundamento de los GLM**  
  (regresiÃ³n logÃ­stica, Poisson, binomial negativa, etc.)

- **ConstrucciÃ³n de intervalos y pruebas estadÃ­sticas**  
  (devianza, test $G^2$, inferencia asintÃ³tica)


## ğŸ“Œ 3. Ejemplo claro

Supongamos datos Bernoulli:
```
1 0 1 1 1 0 (4 Ã©xitos, 2 fracasos)
```

La verosimilitud como funciÃ³n de $p$ es:

$$
L(p)= p^4 (1-p)^2
$$

Buscamos el $p$ que hace mÃ¡s grande esta expresiÃ³n:

$$
\hat{p} = \frac{4}{6}
$$

Ese es el **estimador por mÃ¡xima verosimilitud**.


## ğŸ”¥ 4. Â¿QuÃ© es la **log-verosimilitud**?

Es el **logaritmo natural** de la verosimilitud:

$$
\ell(\theta) = \log L(\theta)
$$

Convierte productos enormes en sumas manejables:

$$
\ell(\theta)= \sum_{i=1}^n \log f(x_i \mid \theta)
$$

Esto se usa siempre porque:
- evita problemas numÃ©ricos,
- simplifica derivadas,
- hace que la teorÃ­a asintÃ³tica funcione mejor.


## ğŸŒŸ 5. Â¿Por quÃ© se usa tanto?

### âœ” Evita underflow numÃ©rico  
Multiplicar probabilidades pequeÃ±as lleva a ceros computacionales.

### âœ” Simplifica derivadas  
Para maximizar, es mucho mÃ¡s fÃ¡cil trabajar con $\ell(\theta)$ que con $L(\theta)$.

### âœ” Es base de criterios modernos  
AIC usa:

$$
AIC = -2\,\ell(\hat\theta) + 2k
$$

TambiÃ©n fundamenta:
- la **devianza** en GLM,  
- el **test de razÃ³n de verosimilitudes**,  
- los **pseudo-RÂ²**,  
- los intervalos basados en curvatura.

### âœ” Conecta con teorÃ­a estadÃ­stica profunda  
La forma de $\ell(\theta)$ determina:
- la varianza del estimador,
- su aproximaciÃ³n normal,
- su eficiencia asintÃ³tica.


## ğŸ§  6. Resumen esencial (para recordar siempre)

- **ParÃ¡metro**: nÃºmero desconocido que describe el modelo.  
- **Verosimilitud**: mide cuÃ¡n compatibles son los datos con un parÃ¡metro.  
- **Log-verosimilitud**: logaritmo natural de $L(\theta)$; mÃ¡s estable y fÃ¡cil.  
- **MLE**: el parÃ¡metro que maximiza la log-verosimilitud.  
- **Base de todo**: AIC, GLM, regresiÃ³n logÃ­stica, devianza, test LRT.

Si entiendes estos conceptos, la regresiÃ³n logÃ­stica, los GLM, el AIC y las pruebas de verosimilitud se vuelven muchÃ­simo mÃ¡s intuitivos.

